{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification of Technical Articles\n",
    "​\n",
    "OBJECTIVE :\n",
    "The objective of this project is to determine whether or not, a set of articles can be classified or clustered into categories.\n",
    "\n",
    "About Dataset:\n",
    "Deskdrop is a collaboration tool that allows employees to communicate and interact with colleagues by interacting with articles. This dataset comprise of Deskdrop data. We have two datasets as follows:\n",
    "1. Shared Articles This dataset contains information about the articles that have been shared on the platform. It has informatin like the published data, URL of the article, title, content and the language of the article.\n",
    "2. User Interaction User Interaction dataset comprises all logs of the user’s interaction with the article. The information of the user such as users’s country, region, ID and which device they are using to open the article. It also has information about the type of interaction as well. Data Dictionary Data Dictionary\n",
    "\n",
    "Target Variable: CategoryID (for Supervised) and ClusterIndex (for Unsupervised) Input variables: title, text\n",
    "\n",
    "# Importing Libraries\n",
    "import pydot\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models, similarities\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "We have two csv files namely ‘shared_articles.csv’ and ‘user_interactions.csv’. The following codes shows the original datasets :\n",
    "\n",
    "#Shared Articles Dataset\n",
    "articlesDF = pd.read_csv(r'C:\\Users\\devan\\Documents\\DWorks\\CIT Deskdrop data set\\shared_articles.csv')\n",
    "#User Interactions Dataset\n",
    "interactionsDF = pd.read_csv(r'C:\\Users\\devan\\Documents\\DWorks\\CIT Deskdrop data set\\users_interactions.csv')\n",
    "Questions to be answered:\n",
    "1.How can we classify the articles based on their category?\n",
    "\n",
    "2.Can we predict the category of the unknown articles with higher precision and accuracy?\n",
    "\n",
    "3.What are the models to be implemented and why are they suitable?\n",
    "\n",
    "articlesDF.dtypes\n",
    "timestamp           int64\n",
    "eventType          object\n",
    "contentId           int64\n",
    "authorPersonId      int64\n",
    "authorSessionId     int64\n",
    "authorUserAgent    object\n",
    "authorRegion       object\n",
    "authorCountry      object\n",
    "contentType        object\n",
    "url                object\n",
    "title              object\n",
    "text               object\n",
    "lang               object\n",
    "dtype: object\n",
    "interactionsDF.dtypes\n",
    "timestamp       int64\n",
    "eventType      object\n",
    "contentId       int64\n",
    "personId        int64\n",
    "sessionId       int64\n",
    "userAgent      object\n",
    "userRegion     object\n",
    "userCountry    object\n",
    "dtype: object\n",
    "Business Understanding:\n",
    "There are many ways an article classification and clustering model can be used. The below chart explains about how classification and clustering are so effective for the business:\n",
    "\n",
    "Business Understanding\n",
    "\n",
    "Data Preparation and Exploratory Data Analysis\n",
    "1. Correction in Data Types:\n",
    "Since IDs are in number form, the data was read as type int64 but all the IDs namely, contentId, authorPersonId, authorSessionId, personId, sessionId are of text type so they are converted to string as the first step in data cleaning.\n",
    "\n",
    "articlesDF.contentId = articlesDF.contentId.astype(str)\n",
    "articlesDF.authorPersonId = articlesDF.authorPersonId.astype(str)\n",
    "articlesDF.authorSessionId = articlesDF.authorSessionId.astype(str)\n",
    "articlesDF.shape\n",
    "(3122, 13)\n",
    "interactionsDF.contentId = interactionsDF.contentId.astype(str)\n",
    "interactionsDF.personId = interactionsDF.personId.astype(str)\n",
    "interactionsDF.sessionId = interactionsDF.sessionId.astype(str)\n",
    "interactionsDF.shape\n",
    "(72312, 8)\n",
    "2. Deletion of removed articles from the dataframes(CONTENT REMOVED):\n",
    "There are two types of articles, (‘CONTENT SHARED’,’CONTENT REMOVED’) in the datasets which is explained by the ‘eventType’ column in the Shared article dataset. Since we cannot use removed articles for prediction and categorization, we remove them from both User interactions and Shared article datasets.\n",
    "\n",
    "# CONTENT REMOVED JUST HAS 75 ENTRIES AND IS CONTENT THAT IS NO LONGER ACCESSIBLE WISE TO DELETE\n",
    "deletedContentID =articlesDF.contentId[articlesDF.eventType == 'CONTENT REMOVED']\n",
    "articlesDF = articlesDF[articlesDF.eventType == 'CONTENT SHARED']\n",
    "articlesDF.drop('eventType', axis=1, inplace=True)\n",
    "interactionsDF = interactionsDF[~interactionsDF.contentId.isin(deletedContentID)]\n",
    "3. Renaming column names before merging dataframes:\n",
    "Since we have timestamp in both dataframe and each timestamp means something different, we rename the timestamp column in dataframes before merging.\n",
    "\n",
    "#before joining rename similar name columns\n",
    "articlesDF.rename(inplace = True, columns={\"timestamp\":\"articlePublishedTime\"})\n",
    "interactionsDF.rename(inplace=True, columns={\"timestamp\": \"articleInteractedTime\"})\n",
    "pie_chart = go.Figure()\n",
    "aLabels = articlesDF['contentType'].value_counts().index\n",
    "aValues = articlesDF['contentType'].value_counts().values\n",
    "pie_chart.add_trace(go.Pie(labels=aLabels,values=aValues))\n",
    "pie_chart.update_traces(textposition='outside', textinfo='percent+label',\\\n",
    "                               marker=dict(colors=['darkorange','gold','red'], line=dict(color='#000000', width=2)))\n",
    "pie_chart.update_layout(height=500, width=800, title_text=\"Distributed Type of Content\")\n",
    "pie_chart.show()\n",
    "Observations\n",
    "HTML Type is the dominating type of content with almost all articles i.e. 99.3% of the data.\n",
    "We can safely remove RICH and VIDEO types since the comprise of a smaller percentage of data\n",
    "4. Removal of contentType feature from Shared article dataset :\n",
    "There are articles of three types of content, namely, HTML, RICH, VIDEO. We will remove RICH and VIDEO type since they comprise of a very small percentage of data.\n",
    "\n",
    "#Delete contents which are not of HTML type and make sure we have the interactions only for those contents which are present\n",
    "deletedContentID =articlesDF.contentId[articlesDF.contentType != 'HTML']\n",
    "articlesDF = articlesDF[articlesDF.contentType == 'HTML']\n",
    "articlesDF.drop('contentType', axis=1, inplace=True)\n",
    "interactionsDF = interactionsDF[~interactionsDF.contentId.isin(deletedContentID)]\n",
    "langPie = go.Figure()\n",
    "aLabels = articlesDF['lang'].value_counts().index\n",
    "aValues = articlesDF['lang'].value_counts().values\n",
    "langPie.add_trace(go.Pie(labels=aLabels, values=aValues))\n",
    "langPie.update_traces(textposition='outside', textinfo='percent+label',\\\n",
    "                               marker=dict(colors=['darkorange','gold','red','yellow','green'], line=dict(color='#000000', width=2)))\n",
    "langPie.update_layout(height=500, width=800, title_text=\"Languages of the Distrtibuted Content\")\n",
    "langPie.show()\n",
    "Observations\n",
    "The data comprises of mainly two languages i.e. portugal (pt) and english (en)\n",
    "Since we plan to classify them, we would only use English articles\n",
    "5. Cleaned userAgent and authorUserAgent into categories :\n",
    "UserAgent and authorUserAgent had browser and device information which was used to access the article. To make it easier to see the devices used by the users, we changed them into set categories.\n",
    "\n",
    "#Create a new column 'authorUserAgentNew' to categorize values for the column 'authorUserAgent' and fill in the missing values\n",
    "userAgentValues = pd.Series([\"Android\", \"Windows\",\"Linux\",\"Mac\",\"iOS\", \"NA\", \"CrOS\"], dtype = \"category\")\n",
    "articlesDF['authorUserAgent'].fillna(\"NA\",inplace=True)\n",
    "articlesDF['authorUserAgentNew'] = userAgentValues\n",
    "for iX, val in userAgentValues.iteritems():\n",
    "    articlesDF.loc[articlesDF.authorUserAgent.str.contains(val, na=False), 'authorUserAgentNew'] = val\n",
    "#articlesDF['authorUserAgentNew'].value_counts()\n",
    "#articlesDF['authorUserAgentNew'].value_counts().sum()\n",
    "barAuthorAgents = go.Figure()\n",
    "barAuthorAgents.add_trace(go.Bar(x=articlesDF['authorUserAgentNew'].value_counts().values,y=articlesDF['authorUserAgentNew'].value_counts().index, orientation='h'))\n",
    "barAuthorAgents.update_layout(height=500, width=800, title_text=\"Distribution of the Devices used by the Authors\")\n",
    "barAuthorAgents.update_yaxes(categoryorder = 'total ascending')\n",
    "barAuthorAgents.show()\n",
    "#Create a new column 'UserAgentNew' to categorize values for the column 'userAgent' and fill in the missing values\n",
    "interactionsDF['userAgentNew'] = userAgentValues\n",
    "interactionsDF['userAgent'].fillna(\"NA\",inplace=True)\n",
    "for iX,val in userAgentValues.iteritems():\n",
    "    interactionsDF.loc[interactionsDF.userAgent.str.contains(val, na=False), 'userAgentNew'] = val\n",
    "​\n",
    "​\n",
    "​\n",
    "barUserAgents = go.Figure()\n",
    "barUserAgents.add_trace(go.Bar(x=interactionsDF['userAgentNew'].value_counts().values,y=interactionsDF['userAgentNew'].value_counts().index,orientation='h'))\n",
    "barUserAgents.update_layout(height=500, width=800, title_text=\"Distribution of the Devices used by the Users\")\n",
    "barUserAgents.update_yaxes(categoryorder = 'total ascending')\n",
    "barUserAgents.show()\n",
    "​\n",
    "#interactionsDF['userAgentNew'].value_counts()\n",
    "#interactionsDF['authorUserAgentNew'].value_counts().sum()\n",
    "articlesDF.drop('authorUserAgent', axis=1, inplace=True)\n",
    "interactionsDF.drop('userAgent', axis=1, inplace=True)\n",
    "articlesDF.rename(inplace = True, columns={\"authorUserAgentNew\":\"authorUserAgent\"})\n",
    "interactionsDF.rename(inplace = True, columns={\"userAgentNew\":\"userAgent\"})\n",
    "Observations\n",
    "As you can see that most of the device information isn't available to us\n",
    "Therefore, we can disregard these features\n",
    "6. Interaction eventType to count of categories in articles:\n",
    "The interaction eventType comprises of all types of interactions that the user did on the articles. The eventTypes are view, like, comment created, bookmark and follow. These were counted from the interactions dataframe and added to the articles dataframe to see the correlation between them.\n",
    "\n",
    "barInteractionType = go.Figure()\n",
    "barInteractionType.add_trace(go.Bar(x=interactionsDF['eventType'].value_counts().index,y=interactionsDF['eventType'].value_counts().values))\n",
    "barInteractionType.update_layout(height=500, width=800, title_text=\"Interaction type of the Users\")\n",
    "barInteractionType.update_xaxes(categoryorder = 'total descending')\n",
    "barInteractionType.show()\n",
    "Observations\n",
    "Most of the articles have interaction as just view and there are a very few percentage of articles that have interactions of other kinds\n",
    "articlesDF[\"viewCount\"]= 0\n",
    "articlesDF[\"likeCount\"]=0\n",
    "articlesDF[\"commentcreatedCount\"]=0\n",
    "articlesDF[\"bookmarkCount\"]=0\n",
    "articlesDF[\"followCount\"]=0\n",
    "articlesDF[\"interactionCount\"]=0\n",
    "​\n",
    "for iX,val in articlesDF.iterrows():\n",
    "    articlesDF.loc[iX,\"viewCount\"]=interactionsDF[(interactionsDF.contentId==val.contentId) \n",
    "                                                  & (interactionsDF.eventType==\"VIEW\")].shape[0]\n",
    "    articlesDF.loc[iX,\"followCount\"]=interactionsDF[(interactionsDF.contentId==val.contentId) \n",
    "                                                    & (interactionsDF.eventType==\"FOLLOW\")].shape[0]\n",
    "    articlesDF.loc[iX,\"commentcreatedCount\"]=interactionsDF[(interactionsDF.contentId==val.contentId) \n",
    "                                                            & (interactionsDF.eventType==\"COMMENT CREATED\")].shape[0]\n",
    "    articlesDF.loc[iX,\"likeCount\"]=interactionsDF[(interactionsDF.contentId==val.contentId) \n",
    "                                                  & (interactionsDF.eventType==\"LIKE\")].shape[0]\n",
    "    articlesDF.loc[iX,\"bookmarkCount\"]=interactionsDF[(interactionsDF.contentId==val.contentId) \n",
    "                                                      & (interactionsDF.eventType==\"BOOKMARK\")].shape[0]\n",
    "    articlesDF.loc[iX,\"interactionCount\"]= interactionsDF[(interactionsDF.contentId==val.contentId)].shape[0]\n",
    "articlesDF.describe()\n",
    "articlePublishedTime\tviewCount\tlikeCount\tcommentcreatedCount\tbookmarkCount\tfollowCount\tinteractionCount\n",
    "count\t3.027000e+03\t3027.000000\t3027.000000\t3027.000000\t3027.000000\t3027.000000\t3027.000000\n",
    "mean\t1.468880e+09\t19.959696\t1.888999\t0.526264\t0.809052\t0.460852\t23.644863\n",
    "std\t7.583642e+06\t27.129692\t3.354211\t1.279595\t2.217639\t1.069327\t32.379479\n",
    "min\t1.459194e+09\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\n",
    "25%\t1.462413e+09\t5.000000\t0.000000\t0.000000\t0.000000\t0.000000\t5.000000\n",
    "50%\t1.467201e+09\t11.000000\t1.000000\t0.000000\t0.000000\t0.000000\t14.000000\n",
    "75%\t1.473968e+09\t25.000000\t2.000000\t0.000000\t1.000000\t0.500000\t30.000000\n",
    "max\t1.488308e+09\t355.000000\t53.000000\t13.000000\t78.000000\t14.000000\t433.000000\n",
    "7. Visualizing Correlation among all interactions with HeatMap\n",
    "Correlation among all the interactions are visualized using heatmap as follows:\n",
    "\n",
    "numericalColumnNames=[\"articlePublishedTime\", \"viewCount\",\"likeCount\",\"commentcreatedCount\", \"bookmarkCount\", \"followCount\", \"interactionCount\"]\n",
    "corrMatrix=articlesDF.corr()\n",
    "fig = go.Figure(go.Heatmap(x=numericalColumnNames,y=numericalColumnNames,z= corrMatrix,colorscale='YlorRd'))\n",
    "fig.update_layout(title='Corelation Among Interactions', \\\n",
    "                  width=100 * len(numericalColumnNames), \\\n",
    "                  height=100 * len(numericalColumnNames))\n",
    "fig.show()\n",
    "Observations\n",
    "The above heatmap shows correlations between the counts like viewcount and likecount , followcount and commentcreatedcount\n",
    "Interactioncount is correlated with everyone since its the total\n",
    "8. EDA on country code:\n",
    "In the dataset we have only country codes like 'BR' and 'US' which we are changing it into keys and values or names such as 'BRA', 'Brazil' and 'USA', 'United States'. Finally visualized the country as choropleth map using country names and interaction count.\n",
    "\n",
    "articlesDF.isnull().sum()\n",
    "articlePublishedTime       0\n",
    "contentId                  0\n",
    "authorPersonId             0\n",
    "authorSessionId            0\n",
    "authorRegion            2360\n",
    "authorCountry           2360\n",
    "url                        0\n",
    "title                      0\n",
    "text                       0\n",
    "lang                       0\n",
    "authorUserAgent            0\n",
    "viewCount                  0\n",
    "likeCount                  0\n",
    "commentcreatedCount        0\n",
    "bookmarkCount              0\n",
    "followCount                0\n",
    "interactionCount           0\n",
    "dtype: int64\n",
    "interactionsDF.isnull().sum()\n",
    "articleInteractedTime        0\n",
    "eventType                    0\n",
    "contentId                    0\n",
    "personId                     0\n",
    "sessionId                    0\n",
    "userRegion               15231\n",
    "userCountry              15220\n",
    "userAgent                    0\n",
    "dtype: int64\n",
    "country_code_dict = {\n",
    "    'BR': ('BRA', 'Brazil'),\n",
    "    'US': ('USA', 'United States'),\n",
    "    'KR': ('KOR', 'South Korea'),\n",
    "    'CA': ('CAN', 'Canada'),\n",
    "    'JP': ('JPN', 'Japan'),\n",
    "    'AU': ('AUS', 'Australia'),\n",
    "    'GB': ('GBR', 'United Kingdom'),\n",
    "    'DE': ('DEU', 'Germany'),\n",
    "    'DE': ('DEU', 'Germany'),\n",
    "    'IE': ('IRL', 'Ireland'),\n",
    "    'IS': ('ISL', 'Iceland'),\n",
    "    'SG': ('SGP', 'Singapore'),\n",
    "    'AR': ('ARG', 'Argentina'),\n",
    "    'PT': ('PRT', 'Portugal'),\n",
    "    'IN': ('IND', 'India'),\n",
    "    'ES': ('ESP', 'Spain'),\n",
    "    'IT': ('ITA', 'Italy'),\n",
    "    'MY': ('MYS', 'Malaysia'),\n",
    "    'CO': ('COL', 'Colombia'),\n",
    "    'CN': ('CHN', 'China'),\n",
    "    'CL': ('CHL', 'Chile'),\n",
    "    'NL': ('NLD', 'Netherlands')\n",
    "}\n",
    "interactionsFromCountryDF = pd.DataFrame(interactionsDF['userCountry'])\n",
    "interactionsFromCountryDF['countryCode'] = interactionsDF['userCountry'].apply(lambda x: country_code_dict[x][0] if x in country_code_dict else None)\n",
    "interactionsFromCountryDF['countryName'] = interactionsDF['userCountry'].apply(lambda x: country_code_dict[x][1] if x in country_code_dict else None)\n",
    "interactionsFromCountryDF\n",
    "userCountry\tcountryCode\tcountryName\n",
    "0\tNaN\tNone\tNone\n",
    "1\tUS\tUSA\tUnited States\n",
    "2\tNaN\tNone\tNone\n",
    "3\tNaN\tNone\tNone\n",
    "4\tNaN\tNone\tNone\n",
    "...\t...\t...\t...\n",
    "72307\tBR\tBRA\tBrazil\n",
    "72308\tBR\tBRA\tBrazil\n",
    "72309\tBR\tBRA\tBrazil\n",
    "72310\tBR\tBRA\tBrazil\n",
    "72311\tBR\tBRA\tBrazil\n",
    "71573 rows × 3 columns\n",
    "\n",
    "interactions_by_country_df = pd.DataFrame(interactionsFromCountryDF.groupby('countryName').size().sort_values(ascending=False).reset_index())\n",
    "interactions_by_country_df.columns = ['countryName', 'No. of Interactions']\n",
    "interactions_by_country_df\n",
    "countryName\tNo. of Interactions\n",
    "0\tBrazil\t50900\n",
    "1\tUnited States\t4601\n",
    "2\tSouth Korea\t239\n",
    "3\tCanada\t217\n",
    "4\tJapan\t142\n",
    "5\tAustralia\t137\n",
    "6\tUnited Kingdom\t22\n",
    "7\tGermany\t19\n",
    "8\tIreland\t14\n",
    "9\tIceland\t12\n",
    "10\tSingapore\t11\n",
    "11\tArgentina\t7\n",
    "12\tPortugal\t6\n",
    "13\tIndia\t3\n",
    "14\tSpain\t3\n",
    "15\tItaly\t2\n",
    "16\tMalaysia\t2\n",
    "17\tNetherlands\t1\n",
    "18\tColombia\t1\n",
    "19\tChina\t1\n",
    "20\tChile\t1\n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations=interactions_by_country_df['countryName'], # Spatial coordinates\n",
    "    z = interactions_by_country_df['No. of Interactions'], \n",
    "    locationmode = 'country names', \n",
    "    autocolorscale=True,\n",
    "    colorbar_title = \"No. of Interactions\",\n",
    "))\n",
    "​\n",
    "fig.update_layout(\n",
    "    title_text = 'Interactions by Country', \n",
    "    geo_scope='world', \n",
    ")\n",
    "​\n",
    "fig.show()\n",
    "Observation\n",
    "Most of the data is from Brasil as shown in the above graph\n",
    "The region and country data have a lot of null values and cannot be used as a feature\n",
    "9. MERGING BOTH DATASETS USER INTERACTION AND SHARED ARTICLES: \n",
    "We merged the datasets into \"interactionswitharticlesDF\" to make analyzing easier in future.\n",
    "\n",
    "# Merge 'interactionsDF' with 'articlesDF'and drop duplicate tuples\n",
    "interactionswitharticlesDF = pd.merge(interactionsDF, articlesDF, how='inner', on='contentId')\n",
    "interactionswitharticlesDF = pd.DataFrame(interactionswitharticlesDF[interactionswitharticlesDF['lang'] == 'en'])\n",
    "interactionswitharticlesDF.drop_duplicates()\n",
    "articleInteractedTime\teventType\tcontentId\tpersonId\tsessionId\tuserRegion\tuserCountry\tuserAgent\tarticlePublishedTime\tauthorPersonId\t...\ttitle\ttext\tlang\tauthorUserAgent\tviewCount\tlikeCount\tcommentcreatedCount\tbookmarkCount\tfollowCount\tinteractionCount\n",
    "0\t1465413032\tVIEW\t-3499919498720038879\t-8845298781299428018\t1264196770339959068\tNaN\tNaN\tNA\t1465309434\t-1032019229384696495\t...\tHiri wants to fix the workplace email problem\tHiri is the latest startup trying to fix email...\ten\tNA\t17\t2\t0\t1\t0\t20\n",
    "1\t1465413046\tVIEW\t-3499919498720038879\t-8845298781299428018\t1264196770339959068\tSP\tBR\tMac\t1465309434\t-1032019229384696495\t...\tHiri wants to fix the workplace email problem\tHiri is the latest startup trying to fix email...\ten\tNA\t17\t2\t0\t1\t0\t20\n",
    "2\t1465907360\tVIEW\t-3499919498720038879\t-108842214936804958\t-2163423831651021975\tSP\tBR\tWindows\t1465309434\t-1032019229384696495\t...\tHiri wants to fix the workplace email problem\tHiri is the latest startup trying to fix email...\ten\tNA\t17\t2\t0\t1\t0\t20\n",
    "3\t1465583412\tVIEW\t-3499919498720038879\t-1443636648652872475\t-3976301106281818872\tSP\tBR\tLinux\t1465309434\t-1032019229384696495\t...\tHiri wants to fix the workplace email problem\tHiri is the latest startup trying to fix email...\ten\tNA\t17\t2\t0\t1\t0\t20\n",
    "4\t1465582468\tVIEW\t-3499919498720038879\t-1443636648652872475\t-3976301106281818872\tSP\tBR\tLinux\t1465309434\t-1032019229384696495\t...\tHiri wants to fix the workplace email problem\tHiri is the latest startup trying to fix email...\ten\tNA\t17\t2\t0\t1\t0\t20\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "71564\t1482951829\tVIEW\t8057379878708189245\t8785971928543012853\t8714595590796310647\tSP\tBR\tWindows\t1482951676\t3609194402293569455\t...\tOculus acquires eye-tracking startup The Eye T...\tThe direction you look could one day control y...\ten\tLinux\t3\t0\t0\t0\t0\t3\n",
    "71565\t1483007295\tVIEW\t8057379878708189245\t3761892846624989548\t7687596359036798986\tSP\tBR\tMac\t1482951676\t3609194402293569455\t...\tOculus acquires eye-tracking startup The Eye T...\tThe direction you look could one day control y...\ten\tLinux\t3\t0\t0\t0\t0\t3\n",
    "71566\t1482510235\tVIEW\t7029834616968294970\t3094513233385472738\t1563742406902490761\tMG\tBR\tWindows\t1466074402\t-2979881261169775358\t...\tCopyrighting DNA Is a Bad Idea\tA few years ago, molecular biologists Jennifer...\ten\tNA\t1\t0\t0\t0\t0\t1\n",
    "71568\t1482929120\tVIEW\t-7108012586837980940\t-4028919343899978105\t1103146045123415934\tSP\tBR\tWindows\t1482929117\t-4028919343899978105\t...\tThe Ultimate Digital Clean-Up Checklist: Are Y...\tWith a couple of days left until New Year's Ev...\ten\tWindows\t1\t0\t0\t0\t0\t1\n",
    "71569\t1483616272\tVIEW\t7526977287801930517\t-3643155458357242906\t8781579964260038140\tMG\tBR\tMac\t1483616270\t-3643155458357242906\t...\tRenewing Medium's focus\tWe've decided to make some major changes at Me...\ten\tMac\t1\t0\t0\t0\t0\t1\n",
    "46631 rows × 24 columns\n",
    "\n",
    "10. Finding distinct articles, user and user sessions:\n",
    "print('Distinct Articles: \\t%d' % len(interactionswitharticlesDF['contentId'].unique()))\n",
    "print('Distinct Users: \\t%d' % len(interactionswitharticlesDF['personId'].unique()))\n",
    "print('Distinct User sessions: \\t%d' % len(interactionswitharticlesDF['sessionId'].unique()))\n",
    "​\n",
    "#The summary shows that 50% of the users have interacted with atleast 6 articles \n",
    "interactionswitharticlesDF.groupby('personId')['contentId'].size().describe()\n",
    "Distinct Articles: \t2094\n",
    "Distinct Users: \t1641\n",
    "Distinct User sessions: \t19106\n",
    "count    1641.000000\n",
    "mean       28.421085\n",
    "std        86.963610\n",
    "min         1.000000\n",
    "25%         2.000000\n",
    "50%         6.000000\n",
    "75%        21.000000\n",
    "max      1623.000000\n",
    "Name: contentId, dtype: float64\n",
    "11. Dropped features and checked for missing values:\n",
    "The following columns has missing values, all of the features are present with str content and have no impact on the target variable, thus all these columns are dropped and the dataset is checked for missing values.\n",
    "\n",
    "interactionswitharticlesDF.drop(columns=['userCountry', 'userRegion', 'userAgent', 'authorRegion', 'authorCountry', 'authorUserAgent'], axis=1, inplace=True)\n",
    "​\n",
    "interactionswitharticlesDF.isnull().sum() \n",
    "articleInteractedTime    0\n",
    "eventType                0\n",
    "contentId                0\n",
    "personId                 0\n",
    "sessionId                0\n",
    "articlePublishedTime     0\n",
    "authorPersonId           0\n",
    "authorSessionId          0\n",
    "url                      0\n",
    "title                    0\n",
    "text                     0\n",
    "lang                     0\n",
    "viewCount                0\n",
    "likeCount                0\n",
    "commentcreatedCount      0\n",
    "bookmarkCount            0\n",
    "followCount              0\n",
    "interactionCount         0\n",
    "dtype: int64\n",
    "12. Converted timestamp values to datetime\n",
    "The timestamp column in the dataset has been converted into their appropriate date and time format to visualize user interaction and shared articles over time.\n",
    "\n",
    "def to_datetime(ts):\n",
    "    return datetime.fromtimestamp(ts)\n",
    "​\n",
    "def to_datetime_str(ts):\n",
    "    return to_datetime(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "interactionswitharticlesDF['articleInteractedTime'] = interactionswitharticlesDF['articleInteractedTime'].apply(lambda x: to_datetime(x))\n",
    "interactionswitharticlesDF['month_x'] = interactionswitharticlesDF['articleInteractedTime'].apply(lambda x: '{0}-{1:02}'.format(x.year, x.month))\n",
    "ploted = interactionswitharticlesDF.groupby('month_x').size().plot(kind='bar', title='User Interactions by Month')\n",
    "interactionswitharticlesDF.drop('month_x', axis=1, inplace=True)\n",
    "ploted.set_xlabel('Month')\n",
    "ploted.set_ylabel('No. of User Interactions')\n",
    "ploted\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x26864d4f550>\n",
    "\n",
    "articlesDF['articlePublishedTime'] = articlesDF['articlePublishedTime'].apply(lambda y: to_datetime(y))\n",
    "interactionswitharticlesDF['articlePublishedTime'] = interactionswitharticlesDF['articlePublishedTime'].apply(lambda y: to_datetime(y))\n",
    "articlesDF['month_y'] = articlesDF['articlePublishedTime'].apply(lambda y: '{0}-{1:02}'.format(y.year, y.month))\n",
    "ploted = articlesDF.groupby('month_y').size().plot(kind='bar', title='Shared Articles by Month')\n",
    "articlesDF.drop('month_y', axis=1, inplace=True)\n",
    "ploted.set_xlabel('Month')\n",
    "ploted.set_ylabel('No. of Shared Articles')\n",
    "ploted\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x26863919c40>\n",
    "\n",
    "13.Main Domains on Shared Articles\n",
    "Domains of the article are seperated\n",
    "\n",
    "urlDomainsDF = pd.DataFrame(interactionswitharticlesDF['url'])\n",
    "urlDomainsDF['urlDomain'] = interactionswitharticlesDF['url'].apply(lambda x: re.sub(r'^http[s]*:\\/\\/', '', re.search(r'^http[s]*:\\/\\/[\\w\\.]*', x, re.IGNORECASE).group(0)))\n",
    "urlDomainsDF[['urlDomain','url']]\n",
    "urlDomain\turl\n",
    "0\ttechcrunch.com\thttp://techcrunch.com/2016/06/07/hiri/\n",
    "1\ttechcrunch.com\thttp://techcrunch.com/2016/06/07/hiri/\n",
    "2\ttechcrunch.com\thttp://techcrunch.com/2016/06/07/hiri/\n",
    "3\ttechcrunch.com\thttp://techcrunch.com/2016/06/07/hiri/\n",
    "4\ttechcrunch.com\thttp://techcrunch.com/2016/06/07/hiri/\n",
    "...\t...\t...\n",
    "71564\ttechcrunch.com\thttps://techcrunch.com/2016/12/28/the-eye-trib...\n",
    "71565\ttechcrunch.com\thttps://techcrunch.com/2016/12/28/the-eye-trib...\n",
    "71566\tnautil.us\thttp://nautil.us/blog/copyrighting-dna-is-a-ba...\n",
    "71568\twww.smashingmagazine.com\thttps://www.smashingmagazine.com/2016/12/digit...\n",
    "71569\tblog.medium.com\thttps://blog.medium.com/renewing-mediums-focus...\n",
    "46639 rows × 2 columns\n",
    "\n",
    "main_domains_df = pd.DataFrame(urlDomainsDF.groupby('urlDomain').size().sort_values(ascending=True))[-20:].reset_index()\n",
    "main_domains_df.columns = ['urlDomain','count']\n",
    "main_domains_df['count']\n",
    "main_domains_df.plot(kind='barh', x='urlDomain', y='count', figsize=(10,10), title='Main Domains on User Shared Articles')\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x2686394b970>\n",
    "\n",
    "Dataset Copy for KMeans\n",
    "We clean the text and title and store it in a seperate database for KMeans\n",
    "\n",
    "#take out only english articles\n",
    "english_articles_df = pd.DataFrame(articlesDF[articlesDF['lang'] == 'en'])\n",
    "english_articles_df.drop(['authorRegion', 'authorCountry', 'authorUserAgent',''], axis=1, inplace=True)\n",
    "articlescopy_df = english_articles_df.copy()\n",
    "articlescopy_df['AnalysisText']= articlescopy_df['title']+\":\"+articlescopy_df['text']\n",
    "# removing everything except alphabets`\n",
    "articlescopy_df['CleanText'] = articlescopy_df['AnalysisText'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# removing short words\n",
    "articlescopy_df['CleanText'] = articlescopy_df['CleanText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# make all text lowercase\n",
    "articlescopy_df['CleanText'] = articlescopy_df['CleanText'].apply(lambda x: x.lower())\n",
    "articlescopy_df.drop(columns=['AnalysisText'],inplace=True)\n",
    "articlescopy_df.rename(inplace = True, columns={\"CleanText\":\"AnalysisText\"})\n",
    "Feature Extraction\n",
    "Feature extraction is one of stages in the information retrieval system that used to extract the unique feature values of a text document. The process of feature extraction can be done by several methods, one of which is Latent Dirichlet Allocation.\n",
    "\n",
    "#FeatureExtraction with Latent Dirchlet Allocation\n",
    "english_articles_content_df = (english_articles_df['title'] + ':' + english_articles_df['text']).tolist()\n",
    "Data Pre-Processing for Feature Extraction\n",
    "Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "All stopwords are removed.\n",
    "Bigrams (set of two words that occur frequently together) are processed from the unigrams.\n",
    "#Loading a set of English stopwords\n",
    "english_stopset = set(stopwords.words('english')).union(\n",
    "                 {\"things\", \"that's\", \"something\", \"take\", \"don't\", \"may\", \"want\", \"you're\", \n",
    "                  \"set\", \"might\", \"says\", \"including\", \"lot\", \"much\", \"said\", \"know\", \n",
    "                  \"good\", \"step\", \"often\", \"going\", \"thing\", \"things\", \"think\",\"go\", \"write\", \"i'm\",\n",
    "                  \"back\", \"actually\", \"better\", \"look\", \"find\", \"right\", \"example\",  \n",
    "                  \"verb\", \"verbs\", \"really\"})\n",
    "#Tokenizing words of articles\n",
    "tokenizer = RegexpTokenizer(r\"(?u)[\\b\\#a-zA-Z][\\w&-_]+\\b\")\n",
    "english_articles_tokens = list(map(lambda d: [token for token in tokenizer.tokenize(d.lower()) if token not in english_stopset], english_articles_content_df))\n",
    "#Processing bigrams from unigrams (sets of two words that frequently occur together)\n",
    "bigram_transformer = models.Phrases(english_articles_tokens)\n",
    "english_articles_unigrams_bigrams_tokens = list(bigram_transformer[english_articles_tokens])\n",
    "Bag of words on the dataset\n",
    "A dictionary is created which contains the list of unique tokens in the corpus and the creation of the dictionary requires to specify two important parameters.\n",
    "\n",
    "[Gensim filter_extremes] :\n",
    "no_below - This parameter filters out words which are too rare to be informative. Filter out tokens that appear in less than no_below documents (absolute number)\n",
    "no_above - This parameter filters out words which are too frequent to be informative. Filter out tokens that appear in more than no_above documents (fraction of total corpus size)\n",
    "\n",
    "[Gensim doc2bow] :\n",
    "For each document we create a dictionary reporting how many words and how many times those words appear. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method.\n",
    "#Creating a dictionary and filtering out too rare and too common tokens\n",
    "english_dictionary = corpora.Dictionary(english_articles_unigrams_bigrams_tokens)\n",
    "english_dictionary.filter_extremes(no_below=5, no_above=0.4, keep_n=None)\n",
    "english_dictionary.compactify()\n",
    "print(english_dictionary)\n",
    "Dictionary(16311 unique tokens: ['actual', 'advocates', 'agreed_upon', 'agreements', 'aim']...)\n",
    "#Processing Bag-of-Words (BoW) for each article\n",
    "english_articles_bow = [english_dictionary.doc2bow(doc) for doc in english_articles_unigrams_bigrams_tokens]\n",
    "Running LDA through Bag of Words :\n",
    "We are going for 5 categories in the document corpus. Some of the parameters we will be tweaking are :\n",
    "\n",
    "num_topics is the number of requested latent topics to be extracted from the training corpus.\n",
    "id2word is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing\n",
    "Passes is the number of training passes through the corpus.\n",
    "#Training the LDA topic model on English articles\n",
    "lda_model = models.LdaModel(english_articles_bow, id2word=english_dictionary, num_topics=5, passes=10, iterations=500)\n",
    "#Processing the topics for each article\n",
    "english_articles_lda = lda_model[english_articles_bow]\n",
    "# Top 5 Keywords for each Category\n",
    "topic_top5words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 5]\n",
    "​\n",
    "df_top5words_stacked = pd.DataFrame(topic_top5words, columns=['topic_id', 'words'])\n",
    "df_top5words = df_top5words_stacked.groupby('topic_id').agg(', '.join)\n",
    "df_top5words.reset_index(level=0,inplace=True)\n",
    "df_top5words.columns = ['CategoryID', 'Keywords']\n",
    "df_top5words\n",
    "CategoryID\tKeywords\n",
    "0\t0\tcode, version, application, java, drupal\n",
    "1\t1\tsystem, database, machine_learning, service, s...\n",
    "2\t2\tteam, product, customers, business, software\n",
    "3\t3\tapp, google, api, users, bot\n",
    "4\t4\tcompany, ai, technology, companies, google\n",
    "Defining Categories\n",
    "Categories inferred using the keywords in each category and their corresponding weights (Category Classification) 0: Programming 1: Machine Learning 2: Software Development 3: Cloud Service 4: Companies\n",
    "\n",
    "for idX, topic in lda_model.print_topics(-1):\n",
    "    print(\"Category: {} \\nWords: {}\".format(idX, topic))\n",
    "    print(\"\\n\")\n",
    "Category: 0 \n",
    "Words: 0.010*\"code\" + 0.003*\"version\" + 0.003*\"application\" + 0.003*\"java\" + 0.003*\"drupal\" + 0.003*\"performance\" + 0.003*\"php\" + 0.003*\"list\" + 0.003*\"image\" + 0.003*\"method\"\n",
    "\n",
    "\n",
    "Category: 1 \n",
    "Words: 0.006*\"system\" + 0.005*\"database\" + 0.005*\"machine_learning\" + 0.004*\"service\" + 0.004*\"systems\" + 0.004*\"event\" + 0.004*\"blockchain\" + 0.004*\"events\" + 0.003*\"services\" + 0.003*\"different\"\n",
    "\n",
    "\n",
    "Category: 2 \n",
    "Words: 0.009*\"team\" + 0.004*\"product\" + 0.004*\"customers\" + 0.004*\"business\" + 0.004*\"software\" + 0.004*\"company\" + 0.004*\"project\" + 0.004*\"digital\" + 0.004*\"companies\" + 0.004*\"teams\"\n",
    "\n",
    "\n",
    "Category: 3 \n",
    "Words: 0.009*\"app\" + 0.007*\"google\" + 0.007*\"api\" + 0.006*\"users\" + 0.005*\"bot\" + 0.004*\"platform\" + 0.004*\"services\" + 0.004*\"create\" + 0.004*\"facebook\" + 0.004*\"google_cloud\"\n",
    "\n",
    "\n",
    "Category: 4 \n",
    "Words: 0.004*\"company\" + 0.004*\"ai\" + 0.004*\"technology\" + 0.004*\"companies\" + 0.003*\"google\" + 0.003*\"research\" + 0.003*\"world\" + 0.003*\"year\" + 0.002*\"apple\" + 0.002*\"still\"\n",
    "\n",
    "\n",
    "Adding Target Feature to the Current Dataset\n",
    "Target Feature 'categoryID' has been added to the dataset for all the interactions with articles with their respective percent contribution & keywords\n",
    "\n",
    "def format_topics_sentences(ldamodel=None, corpus=english_articles_bow, texts=english_articles_unigrams_bigrams_tokens):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "​\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the CategoryID, Percent Contribution and Keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([(round(topic_num,0)), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['CategoryID', 'PercentContribution', 'Keywords']\n",
    "​\n",
    "    # Add original text to the end of the output\n",
    "    return(sent_topics_df)\n",
    "​\n",
    "​\n",
    "df_topic_sent_keywords = format_topics_sentences(ldamodel=lda_model, corpus=english_articles_bow, texts=english_articles_unigrams_bigrams_tokens)\n",
    "​\n",
    "# Format\n",
    "df_topic_sent_keywords.reset_index(drop=True, inplace=True)\n",
    "english_articles_df.reset_index(drop=True, inplace=True)\n",
    "english_articles_df = pd.concat([english_articles_df, df_topic_sent_keywords], axis = 1)\n",
    "english_articles_df\n",
    "articlePublishedTime\tcontentId\tauthorPersonId\tauthorSessionId\turl\ttitle\ttext\tlang\tviewCount\tlikeCount\tcommentcreatedCount\tbookmarkCount\tfollowCount\tinteractionCount\tAnalysisText\tCategoryID\tPercentContribution\tKeywords\n",
    "0\t2016-03-28 15:39:48\t-4110354420726924665\t4340306774493623681\t8940341205206233829\thttp://www.nytimes.com/2016/03/28/business/dea...\tEthereum, a Virtual Currency, Enables Transact...\tAll of this work is still very early. The firs...\ten\t1\t0\t0\t0\t0\t1\tethereum virtual currency enables transactions...\t4.0\t0.6682\tcompany, ai, technology, companies, google, re...\n",
    "1\t2016-03-28 15:42:26\t-7292285110016212249\t4340306774493623681\t8940341205206233829\thttp://cointelegraph.com/news/bitcoin-future-w...\tBitcoin Future: When GBPcoin of Branson Wins O...\tThe alarm clock wakes me at 8:00 with stream o...\ten\t1\t0\t0\t0\t0\t1\tbitcoin future when gbpcoin branson wins over ...\t4.0\t0.6239\tcompany, ai, technology, companies, google, re...\n",
    "2\t2016-03-28 15:47:54\t-6151852268067518688\t3891637997717104548\t-1457532940883382585\thttps://cloudplatform.googleblog.com/2016/03/G...\tGoogle Data Center 360° Tour\tWe're excited to share the Google Data Center ...\ten\t10\t3\t0\t0\t0\t13\tgoogle data center tour excited share google d...\t3.0\t0.5013\tapp, google, api, users, bot, platform, servic...\n",
    "3\t2016-03-28 15:48:17\t2448026894306402386\t4340306774493623681\t8940341205206233829\thttps://bitcoinmagazine.com/articles/ibm-wants...\tIBM Wants to \"Evolve the Internet\" With Blockc...\tThe Aite Group projects the blockchain market ...\ten\t0\t0\t0\t0\t0\t0\twants evolve internet with blockchain technolo...\t1.0\t0.3957\tsystem, database, machine_learning, service, s...\n",
    "4\t2016-03-28 15:48:42\t-2826566343807132236\t4340306774493623681\t8940341205206233829\thttp://www.coindesk.com/ieee-blockchain-oxford...\tIEEE to Talk Blockchain at Cloud Computing Oxf...\tOne of the largest and oldest organizations fo...\ten\t2\t0\t0\t0\t0\t2\tieee talk blockchain cloud computing oxford co...\t4.0\t0.6251\tcompany, ai, technology, companies, google, re...\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "2194\t2017-02-24 07:35:56\t4675505028897335428\t-1393866732742189886\t-1729556941184852519\thttps://medium.mybridge.co/swift-top-10-articl...\tSwift Top 10 Articles For The Past Year (v.2017)\tFor the past year , we've ranked nearly 9,000 ...\ten\t9\t3\t1\t2\t2\t17\tswift articles past year past year ranked near...\t0.0\t0.6134\tcode, version, application, java, drupal, perf...\n",
    "2195\t2017-02-24 09:37:47\t-3295913657316686039\t6960073744377754728\t-8193630595542572738\thttps://thenextweb.com/apps/2017/02/14/amazon-...\tAmazon takes on Skype and GoToMeeting with its...\tAmazon has launched Chime, a video conferencin...\ten\t3\t0\t0\t0\t0\t3\tamazon takes skype gotomeeting with chime vide...\t3.0\t0.9939\tapp, google, api, users, bot, platform, servic...\n",
    "2196\t2017-02-27 14:20:24\t3618271604906293310\t1908339160857512799\t-183341653743161643\thttps://code.org/about/2016\tCode.org 2016 Annual Report\tFebruary 9, 2017 - We begin each year with a l...\ten\t1\t0\t0\t0\t0\t1\tcode annual report february begin each year wi...\t4.0\t0.9925\tcompany, ai, technology, companies, google, re...\n",
    "2197\t2017-02-28 11:51:59\t6607431762270322325\t-1393866732742189886\t2367029511384577082\thttps://www.bloomberg.com/news/articles/2017-0...\tJPMorgan Software Does in Seconds What Took La...\tAt JPMorgan Chase & Co., a learning machine is...\ten\t1\t0\t0\t0\t0\t1\tjpmorgan software does seconds what took lawye...\t4.0\t0.6999\tcompany, ai, technology, companies, google, re...\n",
    "2198\t2017-02-28 13:51:11\t4109618890343020064\t3891637997717104548\t-7416795577834806518\thttps://www.acquia.com/blog/partner/2017-acqui...\tThe 2017 Acquia Partners of the Year\tThe Acquia Partner Awards Program is comprised...\ten\t2\t0\t1\t0\t1\t4\tacquia partners year acquia partner awards pro...\t2.0\t0.9760\tteam, product, customers, business, software, ...\n",
    "2199 rows × 18 columns\n",
    "\n",
    "Feature Importance\n",
    "It is another type of feature selection. You can get the feature importance of each feature of your dataset by using the feature importance property of the model.\n",
    "\n",
    "Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n",
    "\n",
    "Feature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset.\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "X = interactionswitharticlesDF.iloc[:,[2,3,4,6,7]] \n",
    "y = interactionswitharticlesDF.iloc[:,-3] \n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "ExtraTreesClassifier()\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "[0.34922894 0.02647035 0.02412124 0.27374624 0.32643323]\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "Visualizing the Distribution of Categories\n",
    "english_articles_df.CategoryID = english_articles_df.CategoryID.astype(int)\n",
    "english_articles_df[\"CategoryID\"].value_counts()\n",
    "4    786\n",
    "2    481\n",
    "0    451\n",
    "3    294\n",
    "1    187\n",
    "Name: CategoryID, dtype: int64\n",
    "Category = [\"Programming\",\"Machine Learning\", \"Software Development\",\"Cloud Services\", \"Companies\"]\n",
    "english_articles_df[\"CategoryID\"].value_counts().plot(kind='pie', labels=Category, autopct='%1.0f%%', subplots=True, figsize=(8, 8))\n",
    "array([<matplotlib.axes._subplots.AxesSubplot object at 0x0000026805F63070>],\n",
    "      dtype=object)\n",
    "\n",
    "Models\n",
    "K Means Clustering\n",
    "Since the original dataset had no categories and class labels, we decided to implement unsupervised learning in our model\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',max_df = 0.5, smooth_idf=True)\n",
    "X = vectorizer.fit_transform(articlescopy_df['AnalysisText'])\n",
    "kmeans_model=KMeans(n_clusters=56)\n",
    "kmeans_model.fit(X)\n",
    "KMeans(n_clusters=56)\n",
    "Y=kmeans_model.predict(X)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, kmeans_model.labels_, sample_size=1000))\n",
    "Silhouette Coefficient: 0.019\n",
    "Because the Silhouette Coefficient is very low, KMeans algorithm cannot be used to cluster unseen data, hence we try another model for the same\n",
    "\n",
    "Logistic Regression\n",
    "As we have used LDA to do semantic analysis and find categories of the data, we can use a supervised learning aglorithm like Logistic Regression.\n",
    "\n",
    "X = english_articles_df['text'] + \"  \" + english_articles_df['title']\n",
    "y = english_articles_df['CategoryID']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#Calculating the number of rows in our train set\n",
    "len(y_train)\n",
    "1539\n",
    "y_train.value_counts().plot(kind='pie', labels=Category, autopct='%1.0f%%', subplots=True, figsize=(8, 8))\n",
    "array([<matplotlib.axes._subplots.AxesSubplot object at 0x0000026804E15AF0>],\n",
    "      dtype=object)\n",
    "\n",
    "Training the Logistic Regression Classifier\n",
    "In order to train and test the classifier, the first step should be to tokenize and count the number of occurrence of each word that appear in the articles.\n",
    "\n",
    "We use the CountVectorizer() for that. Each term is assigned a unique integer index.\n",
    "\n",
    "Then the counters are transformed to a TF-IDF representation using TfidfTransformer().\n",
    "\n",
    "The last step creates the Logistic Regression classifier. It is worth noting that the default mode for the LogisticRegression() function can only help us classify binary target variables. In order to be able to classify a multi-class problem, we specify multi_class=’multinomial’. Also worth noting is that the only solvers that can be used for a multiclass problem are: newton-cg, sag & lbfgs which are specified using the solver=newton_cg parameter in the LogisticRegression() function.\n",
    "\n",
    "In order to make the training process easier, scikit-learn provides a Pipeline class that behaves like a compound classifier.\n",
    "\n",
    "text_np = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf2', LogisticRegression(solver='newton-cg', multi_class='multinomial')),\n",
    "                         ])\n",
    "text_np = text_np.fit(X_train, y_train)\n",
    "predicted2 = text_np.predict(X_test)\n",
    "metrics.accuracy_score(y_test, predicted2)\n",
    "0.796969696969697\n",
    "print(metrics.classification_report(y_test, predicted2, target_names=sorted(Category)))\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "      Cloud Services       0.87      0.83      0.85       133\n",
    "           Companies       0.94      0.52      0.67        60\n",
    "    Machine Learning       0.86      0.69      0.77       154\n",
    "         Programming       0.84      0.66      0.74        86\n",
    "Software Development       0.72      0.98      0.83       227\n",
    "\n",
    "            accuracy                           0.80       660\n",
    "           macro avg       0.84      0.73      0.77       660\n",
    "        weighted avg       0.82      0.80      0.79       660\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "cnf_matrix = confusion_matrix(y_test, predicted2)\n",
    "print(cnf_matrix)\n",
    "[[110   1   6   3  13]\n",
    " [  5  31   5   1  18]\n",
    " [  2   1 106   5  40]\n",
    " [  9   0   4  57  16]\n",
    " [  1   0   2   2 222]]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix,classes=Category,title='Confusion Matrix of Model')\n",
    "\n",
    "We can see from the metrics that the precision an f1-score are really good for our trained model.\n",
    "\n",
    "The overall accuracy of classification is 79.69%\n",
    "\n",
    "Conclusion\n",
    "Only useful feature for document classification is the text and title of the article. NLP is used since models don't understand text data. Once the input is inline with the models, we decided which model to use. KMeans is used since our dataset is not labeled. But since KMeans isn't providing good results (Silhouette Coefficient of 0.019), we moved on to supervised classification i.e. Logistic Regression. This algorithm was chosen as it allows multinomial classification and proper solver was chosen according to the problem. We used NLP to get the labels. The algorithm showed good accuracy (79.69%) and precision, recall and f1-score are also in high standing. This proves that logistic regression can be clearly used for classifying documents into categories.\n",
    "\n",
    "unseen_text_data = ['Machine Learning is Fun Have you heard people talking about machine learning but only have a fuzzy idea of what that means? Are you tired of nodding your way through conversations with co-workers? Let’s change that! This guide is for anyone who is curious about machine learning but has no idea where to start. I imagine there are a lot of people who tried reading the wikipedia article, got frustrated and gave up wishing someone would just give them a high-level explanation. That’s what this is. The goal is be accessible to anyone — which means that there’s a lot of generalizations. But who cares? If this gets anyone more interested in ML, then mission accomplished.What is machine learning? Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data. For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It’s the same algorithm but it’s fed different training data so it comes up with different classification logic. “Machine learning” is an umbrella term covering lots of these kinds of generic algorithms. Two kinds of Machine Learning Algorithms You can think of machine learning algorithms as falling into one of two main categories — supervised learning and unsupervised learning. The difference is simple, but really important. Supervised Learning Let’s say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there’s a problem — you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don’t have your experience so they don’t know how to price their houses. To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it’s size, neighborhood, etc, and what similar houses have sold for. So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details — number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:Using that training data, we want to create a program that can estimate how much any other house in your area is worth: This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic. To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out. This kind of like having the answer key to a math test with all the arithmetic symbols erased: From this, can you figure out what kind of math problems were on the test? You know you are supposed to “do something” with the numbers on the left to get each answer on the right. In supervised learning, you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type! Unsupervised Learning Let’s go back to our original example with the real estate agent. What if you didn’t know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called unsupervised learning. This is kind of like someone giving you a list of numbers on a sheet of paper and saying “I don’t really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something — good luck!” So what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you’d find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts. Another cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions. Supervised learning is what we’ll focus on for the rest of this post, but that’s not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer. Side note: There are lots of other types of machine learning algorithms. But this is a pretty good place to start. That’s cool, but does being able to estimate the price of a house really count as “learning”? As a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a “feel” for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of Strong AI research is to be able to replicate this ability with computers. But current machine learning algorithms aren’t that good yet — they only work when focused a very specific, limited problem. Maybe a better definition for “learning” in this case is “figuring out an equation to solve a specific problem based on some example data”. Unfortunately “Machine Figuring out an equation to solve a specific problem based on some example data” isn’t really a great name. So we ended up with “Machine Learning” instead. Of course if you are reading this 50 years in the future and we’ve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human. Let’s write that program! So, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further. If you didn’t know anything about machine learning, you’d probably try to write out some basic rules for estimating the price of a house like this:']\n",
    "actual_unseen = [1] #the id for machine learning category\n",
    "predicted_unseen = text_np.predict(unseen_text_data)\n",
    "metrics.accuracy_score(actual_unseen,predicted_unseen)\n",
    "1.0\n",
    "As you can see from above, our model performs well with unseen data as well\n",
    "\n",
    "​\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
